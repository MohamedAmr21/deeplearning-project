{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Libraries for TensorFlow\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow import keras\n",
    "\n",
    "# Library for Transfer Learning\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# Setting the image size for resizing\n",
    "image_size = 28\n",
    "\n",
    "\n",
    "def read_image(class_path, image_file):\n",
    "    \"\"\"\n",
    "    Read and preprocess an image from a specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - class_path: Path to the directory containing the image\n",
    "    - image_file: Filename of the image\n",
    "\n",
    "    Returns:\n",
    "    - image: Preprocessed and resized grayscale image (2D numpy array)\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(class_path, image_file)\n",
    "    image =  cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    return image\n",
    "\n",
    "\n",
    "# Creating empty lists to store class labels and images\n",
    "class_labels_list = []\n",
    "images_list = []\n",
    "\n",
    "# Path to the dataset directory\n",
    "data_directory = './dataset/Validation/'\n",
    "\n",
    "# List of class directories in the dataset\n",
    "# directories = os.listdir(data_directory)\n",
    "directories = ['A','B','C','D','E','F','G']\n",
    "\n",
    "\n",
    "# Loop through each class directory\n",
    "for class_label, class_name in enumerate(directories):\n",
    "    class_path = os.path.join(data_directory, class_name)\n",
    "    \n",
    "    # Loop through each image file in the class directory\n",
    "    for image_file in os.listdir(class_path):\n",
    "        # Check if the file is a JPEG image\n",
    "        if image_file.endswith('.jpg'):\n",
    "            # Read and preprocess the image\n",
    "            image = read_image(class_path, image_file)\n",
    "            \n",
    "            # Append the image and its corresponding class label to the lists\n",
    "            images_list.append(image)\n",
    "            class_labels_list.append(class_label)\n",
    "\n",
    "# Separate data into feature vectors (X) and class labels (y)\n",
    "data_x = images_list\n",
    "data_y = class_labels_list\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1696, 48, 48, 3), (566, 48, 48, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the images into 3 channels as MNIST images are Black and White so have 1 channel\n",
    "\n",
    "X_train = np.dstack([X_train] * 3)\n",
    "X_test = np.dstack([X_test] * 3)\n",
    "\n",
    "# Reshape images as per the tensor format required by tensorflow\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 3)\n",
    "X_test = X_test.reshape (-1, 28, 28, 3)\n",
    "\n",
    "# Resize the images 48*48 as required by VGG16\n",
    "\n",
    "import keras.utils as image\n",
    "\n",
    "X_train = np.asarray([image.img_to_array(image.array_to_img(im, scale=False).resize((48,48))) for im in X_train])\n",
    "X_test = np.asarray([image.img_to_array(image.array_to_img(im, scale=False).resize((48,48))) for im in X_test])\n",
    "#train_x = preprocess_input(x)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kareem\\Desktop\\projects\\aiproj2\\deeplearning-project\\wd.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Freezing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(\"We are making all the layers intrainable except the last layer. \\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# for layer in model.layers[:-1]:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     layer.trainable=False\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,validation_data\u001b[39m=\u001b[39;49m(X_test,y_test))\n",
      "File \u001b[1;32mc:\\Users\\Kareem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Kareem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:985\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    982\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    984\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    988\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[0;32m    989\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    990\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    994\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "\n",
    "# initializing model with weights='imagenet'i.e. we are carring its original weights\n",
    "model_vgg16=VGG16(weights='imagenet')\n",
    "\n",
    "\n",
    "input_layer=layers.Input(shape=(48,48,3))\n",
    "\n",
    "model_vgg16=VGG16(weights='imagenet',input_tensor=input_layer,include_top=False)\n",
    "\n",
    "last_layer=model_vgg16.output \n",
    "\n",
    "flatten=layers.Flatten()(last_layer) \n",
    "\n",
    "\n",
    "layers.Dense(100,activation='relu')(flatten)\n",
    "layers.Dense(100,activation='relu')(flatten)\n",
    "layers.Dense(100,activation='relu')(flatten)\n",
    "\n",
    "output_layer=layers.Dense(10,activation='softmax')(flatten)\n",
    "\n",
    "model=models.Model(inputs=input_layer,outputs=output_layer)\n",
    "\n",
    "\n",
    "# Freezing\n",
    "# print(\"We are making all the layers intrainable except the last layer. \\n\")\n",
    "# for layer in model.layers[:-1]:\n",
    "#     layer.trainable=False\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=20,batch_size=128,verbose=True,validation_data=(X_test,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
