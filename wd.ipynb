{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Libraries for TensorFlow\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow import keras\n",
    "\n",
    "# Library for Transfer Learning\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# Setting the image size for resizing\n",
    "image_size = 32\n",
    "\n",
    "\n",
    "def read_image(class_path, image_file):\n",
    "    \"\"\"\n",
    "    Read and preprocess an image from a specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - class_path: Path to the directory containing the image\n",
    "    - image_file: Filename of the image\n",
    "\n",
    "    Returns:\n",
    "    - image: Preprocessed and resized grayscale image (2D numpy array)\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(class_path, image_file)\n",
    "    image =  cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    return image\n",
    "\n",
    "\n",
    "# Creating empty lists to store class labels and images\n",
    "class_labels_list = []\n",
    "images_list = []\n",
    "\n",
    "# Path to the dataset directory\n",
    "data_directory = './dataset/Validation/'\n",
    "\n",
    "# List of class directories in the dataset\n",
    "# directories = os.listdir(data_directory)\n",
    "directories = ['A','B','C','D','E','F','G']\n",
    "\n",
    "\n",
    "# Loop through each class directory\n",
    "for class_label, class_name in enumerate(directories):\n",
    "    class_path = os.path.join(data_directory, class_name)\n",
    "    \n",
    "    # Loop through each image file in the class directory\n",
    "    for image_file in os.listdir(class_path):\n",
    "        # Check if the file is a JPEG image\n",
    "        if image_file.endswith('.jpg'):\n",
    "            # Read and preprocess the image\n",
    "            image = read_image(class_path, image_file)\n",
    "            \n",
    "            # Append the image and its corresponding class label to the lists\n",
    "            images_list.append(image)\n",
    "            class_labels_list.append(class_label)\n",
    "\n",
    "# Separate data into feature vectors (X) and class labels (y)\n",
    "\n",
    "data_x = images_list\n",
    "data_y = class_labels_list\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2262 images belonging to 7 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (32, 32, 3), y.shape = (1696,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kareem\\Desktop\\projects\\aiproj2\\deeplearning-project\\wd.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m generated_labels_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Loop through the generator and store the generated images and labels\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, labels \u001b[39min\u001b[39;00m datagen\u001b[39m.\u001b[39;49mflow(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Append the generated images and labels to the lists\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     generated_images_list\u001b[39m.\u001b[39mappend(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kareem/Desktop/projects/aiproj2/deeplearning-project/wd.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     generated_labels_list\u001b[39m.\u001b[39mappend(labels)\n",
      "File \u001b[1;32mc:\\Users\\Kareem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:1370\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m   1317\u001b[0m          x,\n\u001b[0;32m   1318\u001b[0m          y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1326\u001b[0m          ignore_class_split\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1327\u001b[0m          subset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1328\u001b[0m   \u001b[39m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \n\u001b[0;32m   1330\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \n\u001b[0;32m   1369\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m   \u001b[39mreturn\u001b[39;00m NumpyArrayIterator(\n\u001b[0;32m   1371\u001b[0m       x,\n\u001b[0;32m   1372\u001b[0m       y,\n\u001b[0;32m   1373\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1374\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1375\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1376\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1377\u001b[0m       seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   1378\u001b[0m       data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m   1379\u001b[0m       save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[0;32m   1380\u001b[0m       save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[0;32m   1381\u001b[0m       save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m   1382\u001b[0m       ignore_class_split\u001b[39m=\u001b[39;49mignore_class_split,\n\u001b[0;32m   1383\u001b[0m       subset\u001b[39m=\u001b[39;49msubset,\n\u001b[0;32m   1384\u001b[0m       dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Kareem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\preprocessing\\image.py:637\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    634\u001b[0m   x_misc \u001b[39m=\u001b[39m []\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(y):\n\u001b[1;32m--> 637\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    638\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mshould have the same length. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    639\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mFound: x.shape = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, y.shape = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    640\u001b[0m                    (np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mshape, np\u001b[39m.\u001b[39masarray(y)\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    641\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(sample_weight):\n\u001b[0;32m    642\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    643\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mshould have the same length. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    644\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mFound: x.shape = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, sample_weight.shape = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    645\u001b[0m                    (np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mshape, np\u001b[39m.\u001b[39masarray(sample_weight)\u001b[39m.\u001b[39mshape))\n",
      "\u001b[1;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (32, 32, 3), y.shape = (1696,)"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255.,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip = True\n",
    "    )\n",
    "aug_data = datagen.flow_from_directory(\n",
    "        data_directory,\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        class_mode='binary',\n",
    "        target_size=(image_size, image_size),\n",
    "        classes={'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6}\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store generated images and labels\n",
    "generated_images_list = []\n",
    "generated_labels_list = []\n",
    "\n",
    "# Loop through the generator and store the generated images and labels\n",
    "for batch, labels in datagen.flow(X_train, y_train, batch_size=32):\n",
    "    # Append the generated images and labels to the lists\n",
    "    generated_images_list.append(batch)\n",
    "    generated_labels_list.append(labels)\n",
    "\n",
    "    # Break the loop after generating the desired number of batches\n",
    "    if len(generated_images_list) >= 32:\n",
    "        break\n",
    "\n",
    "# Concatenate the generated images and labels\n",
    "generated_images = np.concatenate(generated_images_list, axis=0)\n",
    "generated_labels = np.concatenate(generated_labels_list, axis=0)\n",
    "\n",
    "# Print the shape of the generated images and labels\n",
    "print(f'Shape of generated images: {generated_images.shape}')\n",
    "print(f'Shape of generated labels: {generated_labels.shape}')\n",
    "\n",
    "\n",
    "# Concatenate the data\n",
    "concatenated_data = np.concatenate((X_train, generated_images), axis=0)\n",
    "\n",
    "# Concatenate the labels of the original data and augmented data\n",
    "original_labels = y_train\n",
    "concatenated_labels = np.concatenate((original_labels, generated_labels), axis=0)\n",
    "\n",
    "x_train_flatten, x_test_flatten = train_test_split(concatenated_data, test_size=0.2, random_state=42)\n",
    "print(concatenated_data.shape )\n",
    "print(concatenated_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 85)                10965     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 65)                5590      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 45)                2970      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 39)                1794      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,162,887\n",
      "Trainable params: 448,199\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define VGG16 Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "input_shape = (image_size, image_size, 3)\n",
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "base_model.trainable = False #change to true and comment the difference\n",
    "\n",
    "model_vgg16 = tf.keras.Sequential()\n",
    "model_vgg16.add(base_model)\n",
    "model_vgg16.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "# model_vgg16.add(tf.keras.layers.Flatten())\n",
    "model_vgg16.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model_vgg16.add(tf.keras.layers.Dropout(0.5))\n",
    "model_vgg16.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "model_vgg16.add(tf.keras.layers.Dense(85, activation='relu'))\n",
    "model_vgg16.add(tf.keras.layers.Dense(65, activation='relu'))\n",
    "model_vgg16.add(tf.keras.layers.Dense(45, activation='relu'))\n",
    "model_vgg16.add(tf.keras.layers.Dense(39, activation='softmax'))\n",
    "\n",
    "model_vgg16.compile(loss='SparseCategoricalCrossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "              metrics=['acc'])\n",
    "model_vgg16.summary()\n",
    "model_vgg16.save(\"model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 4.7958 - acc: 0.1645WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 6s 100ms/step - loss: 4.7958 - acc: 0.1645 - val_loss: 3.1489 - val_acc: 0.1025\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 3.0351 - acc: 0.2854WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 5s 102ms/step - loss: 3.0351 - acc: 0.2854 - val_loss: 2.4561 - val_acc: 0.1431\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 2.3395 - acc: 0.2919WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 5s 103ms/step - loss: 2.3395 - acc: 0.2919 - val_loss: 1.7728 - val_acc: 0.2138\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.7823 - acc: 0.2052WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 6s 116ms/step - loss: 1.7823 - acc: 0.2052 - val_loss: 1.4209 - val_acc: 0.1219\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.5347 - acc: 0.1527WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 7s 136ms/step - loss: 1.5347 - acc: 0.1527 - val_loss: 1.2257 - val_acc: 0.0565\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.3466 - acc: 0.1474WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 7s 138ms/step - loss: 1.3466 - acc: 0.1474 - val_loss: 1.0973 - val_acc: 0.1184\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.2341 - acc: 0.1521WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 6s 121ms/step - loss: 1.2341 - acc: 0.1521 - val_loss: 0.9891 - val_acc: 0.0901\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.1145 - acc: 0.1515WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 6s 120ms/step - loss: 1.1145 - acc: 0.1515 - val_loss: 0.9214 - val_acc: 0.1290\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 1.0635 - acc: 0.1627WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 6s 122ms/step - loss: 1.0635 - acc: 0.1627 - val_loss: 0.8412 - val_acc: 0.1360\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.9600 - acc: 0.1592WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "53/53 [==============================] - 7s 123ms/step - loss: 0.9600 - acc: 0.1592 - val_loss: 0.8022 - val_acc: 0.0919\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "             filepath='./model.h5',\n",
    "             monitor='val_accuracy',\n",
    "             mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1)\n",
    "early =EarlyStopping(monitor='val_loss', mode='max',restore_best_weights=True, verbose=1, patience=1)\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model_vgg16.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        validation_data = (np.array(X_test), np.array(y_test)),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
